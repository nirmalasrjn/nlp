{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111e0eef-1526-4d8f-95a2-e6db06a36b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import transformers\n",
    "from transformers import MarianMTModel,MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364ba298-b5b8-40aa-b7d2-d697dc6cb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Helsinki-NLP/opus-mt-en-vi Model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-es-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dcb71e-feaa-4fd6-8c3d-05d77d137774",
   "metadata": {},
   "source": [
    "### Input a single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751b54b9-eab7-47f6-aa57-06306c61e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input single sentence \n",
    "input_text=\"Todavia recuerdo aquel amanecer en que mi padre me llevo por primera vez a visitar el Cemeterio de los Libros Ollvidados\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400f9beb-5e0f-47fe-9e8b-7db45adeb5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 7126,  3274,  8733,  5775, 29736,    12,    15,   155,  1619,    74,\n",
      "         19390,    36,   749,   259,     8,  4998,    14,  6289, 18878,  3187,\n",
      "             4,    17, 27735,   425,   210, 19350,   503,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text ( convert source language into input tokens)\n",
    "# the input tokens are returned in a format that can be used by the model directly. \n",
    "# In the output the input_ids represent a word or subword from the sentence\n",
    " \n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\",padding=True)\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43490b55-36bb-4eb1-98da-fbc9c7ac8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65000,    33,   616,  2743,    27, 24423,   208,   125,  2110,  1231,\n",
      "            74,    23,     5,   269,   158,    13,  2080,     5, 55221,     7,\n",
      "             5, 21778,  1513, 23462,     0]])\n"
     ]
    }
   ],
   "source": [
    "# Generate token IDs in the target language using the input tokens of source language\n",
    "translated=model.generate(**(input_tokens))\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb349a40-30ed-44b3-b3e5-34244361ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: ['I still remember that dawn when my father took me for the first time to visit the Cemetery of the Forgotten Books']\n"
     ]
    }
   ],
   "source": [
    "# Decode : Convert the translated token IDs back to text\n",
    "output_text=[tokenizer.decode(t,skip_special_tokens=True) for t in translated]\n",
    "print(\"Translated Text:\", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd952198-e332-4b84-8ac3-35eb6717d661",
   "metadata": {},
   "source": [
    "### Multiple Input Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06802581-7884-46a8-bcfd-9cdaf540dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding ensures that all sentences in a batch are of same length which is a requirement for batch processing using transformers.\n",
    "\n",
    "input_text=\"Todavia recuerdo aquel amanecer en que mi padre me llevo por primera vez a visitar el Cemeterio de los Libros Ollvidados\", \" A ella puedes contarselo todo\", \"Poco despues de la guerra civil, un brote de colera se habia llevado a mi madre.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f7ac3c9-1dc2-47bc-8cc9-bb2c26de6cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 7126,  3274,  8733,  5775, 29736,    12,    15,   155,  1619,    74,\n",
      "         19390,    36,   749,   259,     8,  4998,    14,  6289, 18878,  3187,\n",
      "             4,    17, 27735,   425,   210, 19350,   503,     0],\n",
      "        [   70,   668,  1534,  4120, 24728,   163,     0, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000],\n",
      "        [15876, 28786,     4,     6,  1662,   647,     2,    28, 35847,     4,\n",
      "          7624,  2910,    26, 30638,  6130,     8,   155,  1898,     3,     0,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\",padding=True)\n",
    "print(input_tokens)\n",
    "# In the output, the attention_mask:has values 0 and 1. \n",
    "# Attention mask of 0 indicates that it is a padding mask and should be ignored.\n",
    "# Attention mask of 1 indicates that the sequence has to be attached.\n",
    "# When a single input sentence was given the input tokens had attention mask walue of 1 only\n",
    "# When two input texts of differing length was given, we get mask values of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf500218-0c75-4f75-8413-307c886e867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65000,    33,   616,  2743,    27, 24423,   208,   125,  2110,  1231,\n",
      "            74,    23,     5,   269,   158,    13,  2080,     5, 55221,     7,\n",
      "             5, 21778,  1513, 23462,     0],\n",
      "        [65000,    99,    88,   922,   225,  1645,     3,     0, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000],\n",
      "        [65000, 42912,   421,     5,   647,  1733,     2,    76, 21081,     7,\n",
      "          7624,  2910,   115,   621,   125,  2195,     3,     0, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000]])\n"
     ]
    }
   ],
   "source": [
    "# Generate token IDs in the target language using the input tokens of source language\n",
    "translated=model.generate(**(input_tokens))\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "573aa8f1-11be-4502-b40a-9d52f13e76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode: Convert the translated token IDs back to text\n",
    "output_text=[tokenizer.decode(t,skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf1e81e0-4242-4ee7-ad21-e2f2a872c707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: ['I still remember that dawn when my father took me for the first time to visit the Cemetery of the Forgotten Books', 'You can tell her everything.', 'Shortly after the civil war, an outbreak of colera had taken my mother.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Translated Text:\", output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
